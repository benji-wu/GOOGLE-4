import re
import pdfplumber
import chromadb
from chromadb.config import Settings
from chromadb.utils.embedding_functions import EmbeddingFunction
import google.generativeai as genai
from google.generativeai.types import GenerationConfig


# âœ… è¨­å®š Gemini API é‡‘é‘°
genai.configure(api_key="YOUR_GEMINI_API_KEY")  # â† è«‹æ›¿æ›


# âœ… Gemini åµŒå…¥å‡½æ•¸
class GeminiEmbeddingFunction(EmbeddingFunction):
    def __call__(self, texts):
        if isinstance(texts, str):
            texts = [texts]

        embeddings = []
        for text in texts:
            response = genai.embed_content(
                model="models/embedding-001",
                content=text,
                task_type="retrieval_document"
            )
            embeddings.append(response["embedding"])
        return embeddings


# âœ… è®€å– PDF æ–‡å­—
def extract_text_from_pdf(pdf_path: str) -> str:
    full_text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                full_text += page_text + "\n"
    return full_text.strip()


# âœ… åˆ†æ®µï¼ˆæŒ‰èªæ„å¥å­ + overlapï¼‰
def split_text(text: str, max_chunk_size=500, overlap=100) -> list:
    text = re.sub(r'\s+', ' ', text).strip()
    sentence_delimiters = re.compile(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\s')
    sentences = sentence_delimiters.split(text)

    chunks = []
    current_chunk = ""

    for sentence in sentences:
        if len(current_chunk) + len(sentence) + 1 <= max_chunk_size:
            current_chunk += sentence + " "
        else:
            chunks.append(current_chunk.strip())
            if overlap > 0:
                current_chunk = current_chunk[-overlap:] + sentence + " "
            else:
                current_chunk = sentence + " "

    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks


# âœ… å»ºç«‹ ChromaDB
def create_chroma_db(documents, path="./chroma_db", name="pdf_chunks"):
    client = chromadb.Client(Settings(chroma_db_impl="duckdb+parquet", persist_directory=path))
    embedding_function = GeminiEmbeddingFunction()
    collection = client.get_or_create_collection(name=name, embedding_function=embedding_function)

    ids = [f"doc_{i}" for i in range(len(documents))]
    collection.add(documents=documents, ids=ids)
    client.persist()
    return collection


# âœ… æŸ¥è©¢æœ€ç›¸é—œæ®µè½
def get_relevant_passage(query: str, db, n_results: int = 3) -> list:
    results = db.query(query_texts=[query], n_results=n_results)
    return results["documents"][0]


# âœ… å»ºç«‹ promptï¼ˆRAGï¼‰
def make_rag_prompt(query: str, relevant_passages: list) -> str:
    context = "\n\n".join(relevant_passages)
    prompt = (
        "Based on the following information:\n\n"
        f"{context}\n\n"
        "Please answer this question:\n"
        f"{query}"
    )
    return prompt


# âœ… å›ç­”ç”Ÿæˆï¼ˆæ”¯æ´æº«åº¦èˆ‡ token æ§åˆ¶ï¼‰
def generate_answer(prompt: str, temperature: float = 0.7, max_tokens: int = 512) -> str:
    config = GenerationConfig(
        temperature=temperature,
        max_output_tokens=max_tokens
    )
    response = genai.generate_content(
        model="models/gemini-pro",
        contents=[{"role": "user", "parts": [prompt]}],
        generation_config=config
    )
    return response.text.strip()


# âœ… ä¸»ç¨‹å¼ï¼ˆå« temperature å¯¦é©—ï¼‰
if __name__ == "__main__":
    pdf_path = "your_file.pdf"  # â† æ›¿æ›ç‚ºä½ çš„ PDF è·¯å¾‘
    full_text = extract_text_from_pdf(pdf_path)
    print("âœ… PDF è®€å–å®Œæˆ")

    chunks = split_text(full_text)
    print(f"âœ… åˆ†å‰²ç‚º {len(chunks)} å€‹èªæ„æ®µè½")

    collection = create_chroma_db(chunks, path="./chroma_db", name="example_pdf")
    print("âœ… Chroma å‘é‡è³‡æ–™åº«å»ºç«‹å®Œæˆ")

    while True:
        user_question = input("\nâ“ è«‹è¼¸å…¥å•é¡Œï¼ˆæˆ–è¼¸å…¥ 'exit' çµæŸï¼‰ï¼š")
        if user_question.lower() == 'exit':
            break

        top_chunks = get_relevant_passage(user_question, collection, n_results=3)
        prompt = make_rag_prompt(user_question, top_chunks)

        print("\nğŸ§ª Gemini å›ç­”æ¯”è¼ƒï¼ˆä¸åŒ temperatureï¼‰ï¼š")
        for temp in [0.0, 0.5, 1.0]:
            print(f"\nğŸŒ¡ï¸ Temperature = {temp}")
            try:
                answer = generate_answer(prompt, temperature=temp, max_tokens=300)
                print(f"ğŸ¤– å›ç­”:\n{answer}")
            except Exception as e:
                print(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
