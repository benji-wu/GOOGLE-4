import re
import pdfplumber
import chromadb
from chromadb.config import Settings
from chromadb.utils.embedding_functions import EmbeddingFunction
import google.generativeai as genai
from google.generativeai.types import GenerationConfig

# âœ… è¨­å®š Gemini API é‡‘é‘°
genai.configure(api_key="YOUR_GEMINI_API_KEY")  # â† è«‹æ›¿æ›

# âœ… Gemini åµŒå…¥å‡½æ•¸
class GeminiEmbeddingFunction(EmbeddingFunction):
    def __call__(self, texts):
        if isinstance(texts, str):
            texts = [texts]
        embeddings = []
        for text in texts:
            response = genai.embed_content(
                model="models/embedding-001",
                content=text,
                task_type="retrieval_document"
            )
            embeddings.append(response["embedding"])
        return embeddings

# âœ… PDF è½‰æ–‡å­—
def extract_text_from_pdf(pdf_path: str) -> str:
    full_text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                full_text += page_text + "\n"
    return full_text.strip()

# âœ… åˆ†æ®µåˆ‡ç‰‡
def split_text(text: str, max_chunk_size=500, overlap=100) -> list:
    text = re.sub(r'\s+', ' ', text).strip()
    sentence_delimiters = re.compile(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\s')
    sentences = sentence_delimiters.split(text)

    chunks = []
    current_chunk = ""

    for sentence in sentences:
        if len(current_chunk) + len(sentence) + 1 <= max_chunk_size:
            current_chunk += sentence + " "
        else:
            chunks.append(current_chunk.strip())
            if overlap > 0:
                current_chunk = current_chunk[-overlap:] + sentence + " "
            else:
                current_chunk = sentence + " "
    if current_chunk:
        chunks.append(current_chunk.strip())
    return chunks

# âœ… å»ºç«‹ Chroma å‘é‡è³‡æ–™åº«
def create_chroma_db(documents, path="./chroma_db", name="pdf_chunks"):
    client = chromadb.Client(Settings(chroma_db_impl="duckdb+parquet", persist_directory=path))
    embedding_function = GeminiEmbeddingFunction()
    collection = client.get_or_create_collection(name=name, embedding_function=embedding_function)
    ids = [f"doc_{i}" for i in range(len(documents))]
    collection.add(documents=documents, ids=ids)
    client.persist()
    return collection

# âœ… æŸ¥è©¢ç›¸é—œæ®µè½
def get_relevant_passage(query: str, db, n_results: int = 3) -> list:
    results = db.query(query_texts=[query], n_results=n_results)
    return results["documents"][0]

# âœ… å»ºç«‹ RAG Prompt
def make_rag_prompt(query: str, relevant_passages: list) -> str:
    context = "\n\n".join(relevant_passages)
    prompt = (
        "Based on the following information:\n\n"
        f"{context}\n\n"
        "Please answer this question:\n"
        f"{query}"
    )
    return prompt

# âœ… å›ç­”ç”Ÿæˆï¼ˆæ”¯æ´æº«åº¦èˆ‡ tokenï¼‰
def generate_answer(prompt: str, temperature: float = 0.7, max_tokens: int = 512) -> str:
    config = GenerationConfig(
        temperature=temperature,
        max_output_tokens=max_tokens
    )
    response = genai.generate_content(
        model="models/gemini-pro",
        contents=[{"role": "user", "parts": [prompt]}],
        generation_config=config
    )
    return response.text.strip()

# âœ… ä¸»æµç¨‹æ•´åˆ
if __name__ == "__main__":
    pdf_path = "your_file.pdf"  # â† æ›¿æ›ç‚ºä½ çš„ PDF æª”å
    full_text = extract_text_from_pdf(pdf_path)
    print("âœ… PDF è®€å–å®Œæˆ")

    chunks = split_text(full_text)
    print(f"âœ… åˆ†å‰²ç‚º {len(chunks)} å€‹èªæ„æ®µè½")

    collection = create_chroma_db(chunks, path="./chroma_db", name="example_pdf")
    print("âœ… å‘é‡è³‡æ–™åº«å»ºç«‹å®Œæˆ")

    while True:
        user_question = input("\nâ“ è«‹è¼¸å…¥å•é¡Œï¼ˆæˆ–è¼¸å…¥ 'exit' çµæŸï¼‰ï¼š")
        if user_question.lower() == 'exit':
            break

        top_chunks = get_relevant_passage(user_question, collection, n_results=3)
        prompt = make_rag_prompt(user_question, top_chunks)

        # âœ… Task 2: ä¸åŒ temperature å¯¦é©—
        print("\nğŸ§ª æ¯”è¼ƒä¸åŒ Temperatureï¼ˆmax_tokens å›ºå®šç‚º 300ï¼‰ï¼š")
        for temp in [0.0, 0.5, 1.0]:
            print(f"\nğŸŒ¡ï¸ Temperature = {temp}")
            try:
                answer = generate_answer(prompt, temperature=temp, max_tokens=300)
                print(f"ğŸ¤– å›ç­”ï¼š\n{answer}")
            except Exception as e:
                print(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

        # âœ… Task 3: ä¸åŒ max_tokens å¯¦é©—
        print("\nğŸ”¢ æ¯”è¼ƒä¸åŒ max_tokensï¼ˆTemperature å›ºå®šç‚º 0.5ï¼‰ï¼š")
        for tokens in [20, 50, 200]:
            print(f"\nğŸ“ Max Tokens = {tokens}")
            try:
                answer = generate_answer(prompt, temperature=0.5, max_tokens=tokens)
                print(f"ğŸ¤– å›ç­”ï¼š\n{answer}")
            except Exception as e:
                print(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
