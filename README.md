import re
import pdfplumber
import chromadb
from chromadb.config import Settings
from chromadb.utils.embedding_functions import EmbeddingFunction
import google.generativeai as genai
from google.generativeai.types import GenerationConfig

# âœ… è¨­å®š Gemini API é‡‘é‘°
genai.configure(api_key="YOUR_GEMINI_API_KEY")  # â† æ›¿æ›æˆä½ çš„é‡‘é‘°

# âœ… Gemini åµŒå…¥å‡½æ•¸
class GeminiEmbeddingFunction(EmbeddingFunction):
    def __call__(self, texts):
        if isinstance(texts, str):
            texts = [texts]
        embeddings = []
        for text in texts:
            response = genai.embed_content(
                model="models/embedding-001",
                content=text,
                task_type="retrieval_document"
            )
            embeddings.append(response["embedding"])
        return embeddings

# âœ… PDF æ“·å–æ–‡å­—
def extract_text_from_pdf(pdf_path: str) -> str:
    full_text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                full_text += page_text + "\n"
    return full_text.strip()

# âœ… åˆ†æ®µï¼šèªæ„å¥åˆ‡å¡Š
def split_text(text: str, max_chunk_size=500, overlap=100) -> list:
    text = re.sub(r'\s+', ' ', text).strip()
    sentence_delimiters = re.compile(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\s')
    sentences = sentence_delimiters.split(text)

    chunks = []
    current_chunk = ""

    for sentence in sentences:
        if len(current_chunk) + len(sentence) + 1 <= max_chunk_size:
            current_chunk += sentence + " "
        else:
            chunks.append(current_chunk.strip())
            if overlap > 0:
                current_chunk = current_chunk[-overlap:] + sentence + " "
            else:
                current_chunk = sentence + " "
    if current_chunk:
        chunks.append(current_chunk.strip())
    return chunks

# âœ… å»ºç«‹å‘é‡è³‡æ–™åº«
def create_chroma_db(documents, path="./chroma_db", name="pdf_chunks"):
    client = chromadb.Client(Settings(chroma_db_impl="duckdb+parquet", persist_directory=path))
    embedding_function = GeminiEmbeddingFunction()
    collection = client.get_or_create_collection(name=name, embedding_function=embedding_function)
    ids = [f"doc_{i}" for i in range(len(documents))]
    collection.add(documents=documents, ids=ids)
    client.persist()
    return collection

# âœ… æŸ¥è©¢ç›¸é—œ passage
def get_relevant_passage(query: str, db, n_results: int = 3) -> list:
    results = db.query(query_texts=[query], n_results=n_results)
    return results["documents"][0]

# âœ… RAG Prompt çµ„åˆ
def make_rag_prompt(query: str, relevant_passages: list) -> str:
    context = "\n\n".join(relevant_passages)
    prompt = (
        "Based on the following information:\n\n"
        f"{context}\n\n"
        "Please answer this question:\n"
        f"{query}"
    )
    return prompt

# âœ… ç”¢ç”Ÿå›ç­”
def generate_answer(prompt: str, temperature: float = 0.7, max_tokens: int = 512) -> str:
    config = GenerationConfig(
        temperature=temperature,
        max_output_tokens=max_tokens
    )
    response = genai.generate_content(
        model="models/gemini-pro",
        contents=[{"role": "user", "parts": [prompt]}],
        generation_config=config
    )
    return response.text.strip()

# âœ… æ’°å¯« findings.txt
def write_findings():
    findings = """
Findings: Temperature and Token Limit Experiments
==================================================

1. How did the output change as you increased the temperature?
--------------------------------------------------------------
As temperature increased from 0.0 to 1.0, the responses became more diverse, expressive, and sometimes more verbose. 
At temperature 0.0, the answer was deterministic, short, and factual. At 0.5, the language was balanced. 
At 1.0, the answer was more creative and descriptive.

2. When would you prefer a low temperature versus a high one?
-------------------------------------------------------------
- Low temperature (0.0): for factual, deterministic, and reproducible answers (e.g., answering exam questions).
- High temperature (1.0): for storytelling, summaries, or brainstorming.

3. What happened when the max_output_tokens limit was reached?
---------------------------------------------------------------
The response was cut off once the token limit was reached. At very low limits (e.g., 20), the answer was incomplete.
This proves useful to control output size and avoid unnecessary API costs.

4. Practical scenario for setting a token limit?
------------------------------------------------
If integrating answers into an SMS system or UI card, we may need to limit text to 160 tokens or fewer for design purposes.

5. How to get a short, creative response vs a long, factual one?
----------------------------------------------------------------
- Short, creative: temperature=1.0, max_tokens=50
- Long, factual: temperature=0.0 or 0.3, max_tokens=300+
"""
    with open("findings.txt", "w", encoding="utf-8") as f:
        f.write(findings.strip())
    print("\nğŸ“ findings.txt å·²å»ºç«‹")

# âœ… ä¸»ç¨‹å¼å…¥å£
if __name__ == "__main__":
    pdf_path = "your_file.pdf"  # â† æ›¿æ›ç‚ºä½ çš„ PDF æª”æ¡ˆ
    full_text = extract_text_from_pdf(pdf_path)
    print("âœ… PDF è®€å–å®Œæˆ")

    chunks = split_text(full_text)
    print(f"âœ… åˆ†å‰²ç‚º {len(chunks)} å€‹èªæ„æ®µè½")

    collection = create_chroma_db(chunks, path="./chroma_db", name="example_pdf")
    print("âœ… å‘é‡è³‡æ–™åº«å»ºç«‹å®Œæˆ")

    while True:
        user_question = input("\nâ“ è«‹è¼¸å…¥å•é¡Œï¼ˆæˆ–è¼¸å…¥ 'exit' çµæŸï¼‰ï¼š")
        if user_question.lower() == 'exit':
            break

        top_chunks = get_relevant_passage(user_question, collection, n_results=3)
        prompt = make_rag_prompt(user_question, top_chunks)

        # âœ… Task 2: æ¸¬è©¦ä¸åŒ Temperature
        print("\nğŸ§ª æ¯”è¼ƒ Temperatureï¼ˆmax_tokens = 300ï¼‰")
        for temp in [0.0, 0.5, 1.0]:
            print(f"\nğŸŒ¡ï¸ Temperature = {temp}")
            answer = generate_answer(prompt, temperature=temp, max_tokens=300)
            print(f"ğŸ¤– å›ç­”ï¼š\n{answer}")

        # âœ… Task 3: æ¸¬è©¦ä¸åŒ Max Tokens
        print("\nğŸ”¢ æ¯”è¼ƒ Max Tokensï¼ˆtemperature = 0.5ï¼‰")
        for tokens in [20, 50, 200]:
            print(f"\nğŸ“ Max Tokens = {tokens}")
            answer = generate_answer(prompt, temperature=0.5, max_tokens=tokens)
            print(f"ğŸ¤– å›ç­”ï¼š\n{answer}")

        # âœ… Task 4: å¯«å…¥çµæœå ±å‘Š
        write_findings()
